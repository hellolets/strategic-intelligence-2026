"""
M√≥dulo Reporter: Genera reportes finales en Markdown.

MEJORA 2026-01: Chunking inteligente de fuentes
- Prioriza fuentes por score de relevancia
- Incluye fuentes completas hasta llenar el l√≠mite
- Solo trunca la √∫ltima fuente si es necesario
"""
import time
from typing import List, Dict, Optional, Tuple, Any
from .config import (
    llm_analyst,
    llm_analyst_fast,
    llm_analyst_precision,
    model_config,
    TOML_CONFIG,
    USE_DEEPSEEK_FOR_TESTING,
    USE_CHEAP_OPENROUTER_MODELS,
    get_model_limits
)
from .prompts import reporter_prompt
from .utils import count_tokens

# Configuraci√≥n global
REPORT_LANGUAGE = TOML_CONFIG["general"].get("report_language", "Espa√±ol")
REFERENCES_STYLE = TOML_CONFIG["references"].get("style", "IEEE")


# ==========================================
# CHUNKING INTELIGENTE DE FUENTES
# ==========================================

def _get_source_score(source: Dict) -> float:
    """Obtiene el score de relevancia de una fuente."""
    try:
        # Intentar varios campos donde podr√≠a estar el score
        return float(source.get('total_score', source.get('relevance_score', source.get('score', 5.0))))
    except (ValueError, TypeError):
        return 5.0  # Score por defecto


def _chunk_sources_by_relevance(
    sources: List[Dict],
    max_total_tokens: int,
    model_name: str = "gpt-4",
    content_field: str = 'raw_content',
    fallback_field: str = 'snippet'
) -> Tuple[str, int, int]:
    """
    Chunking inteligente: prioriza fuentes por relevancia.
    En lugar de truncar uniformemente todas las fuentes,
    incluye fuentes COMPLETAS ordenadas por score hasta llenar el l√≠mite.

    Args:
        sources: Lista de fuentes con score
        max_total_tokens: L√≠mite m√°ximo de tokens para todas las fuentes
        model_name: Nombre del modelo para contar tokens
        content_field: Campo principal de contenido
        fallback_field: Campo de fallback si no hay contenido principal

    Returns:
        Tuple (sources_text, tokens_usados, fuentes_incluidas_completas)
    """
    if not sources:
        return "", 0, 0

    # 1. Ordenar fuentes por score de relevancia (mayor primero)
    sorted_sources = sorted(sources, key=_get_source_score, reverse=True)

    # deduplicate sorted_sources by URL
    unique_sorted = []
    seen = set()
    for s in sorted_sources:
        u = s.get('url', '').rstrip('/').lower()
        if u and u not in seen:
            unique_sorted.append(s)
            seen.add(u)
        elif not u:
            unique_sorted.append(s)

    # 2. Incluir fuentes completas hasta llenar el l√≠mite
    sources_text_parts = []
    tokens_used = 0
    sources_included_complete = 0
    max_chars_per_source = 15000  # L√≠mite por fuente individual (para evitar fuentes gigantes)

    for i, source in enumerate(unique_sorted):
        content = source.get(content_field, source.get(fallback_field, 'N/A'))
        if not content or content == 'N/A':
            content = source.get(fallback_field, 'N/A')

        # Limitar contenido individual
        content = content[:max_chars_per_source] if content else 'N/A'

        # Formatear la fuente
        source_text = (
            f"[{i+1}] T√≠tulo: {source.get('title', 'N/A')}\n"
            f"URL: {source.get('url', 'N/A')}\n"
            f"Score: {_get_source_score(source):.1f}\n"
            f"Contenido: {content}"
        )

        source_tokens = count_tokens(source_text, model_name)

        # Si a√±adir esta fuente completa no excede el l√≠mite, a√±adirla
        if tokens_used + source_tokens <= max_total_tokens:
            sources_text_parts.append(source_text)
            tokens_used += source_tokens
            sources_included_complete += 1
        else:
            # Si no cabe completa, intentar truncar solo esta fuente
            remaining_tokens = max_total_tokens - tokens_used
            if remaining_tokens > 500:  # Solo si hay espacio significativo
                # Calcular cu√°ntos caracteres podemos incluir
                chars_per_token = len(content) / count_tokens(content, model_name) if count_tokens(content, model_name) > 0 else 4
                available_chars = int(remaining_tokens * chars_per_token * 0.8)  # 80% de margen
                truncated_content = content[:available_chars] + "\n[... contenido truncado ...]"

                truncated_source_text = (
                    f"[{i+1}] T√≠tulo: {source.get('title', 'N/A')}\n"
                    f"URL: {source.get('url', 'N/A')}\n"
                    f"Score: {_get_source_score(source):.1f}\n"
                    f"Contenido: {truncated_content}"
                )

                sources_text_parts.append(truncated_source_text)
                tokens_used += count_tokens(truncated_source_text, model_name)

            # Parar aqu√≠ - las siguientes fuentes ya no caben
            break

    sources_text = "\n\n".join(sources_text_parts)

    return sources_text, tokens_used, sources_included_complete


def _clean_report_metadata(report: str) -> str:
    """
    Limpia metadatos, badges y mensajes de proceso del reporte generado por el LLM.
    El objetivo es obtener SOLO el contenido limpio del reporte.
    """
    import re
    
    # Patrones de metadatos a eliminar
    patterns_to_remove = [
        # Badges de Confidence Score e info de sistema
        r'>\s*[üü¢üü°üü†üî¥‚ö´‚ö™]\s*\*\*Confidence Score:.*?\*\*.*?\n(?:>.*?\n)*',
        r'Confidence Score:\s*\d+/\d+.*?\n',
        # Mensajes de proceso del LLM (comunes en modelos de razonamiento)
        r"Here is the report.*?:",
        r"Here is the drafted report.*?:",
        r"I have analyzed the sources.*?:",
        r"Based on the provided sources.*?:",
        r"Below is the comprehensive report.*?:",
        r"Drafting the report.*?:",
        r"Process:",
        r"Step \d+:",
        r"\*\*Process:\*\*",
        r"\*\*Step \d+:\*\*",
        # Bloques de pensamiento (ocultos o expl√≠citos)
        r'<thinking>.*?</thinking>',
        r'```thinking.*?```',
    ]
    
    cleaned_report = report
    for pattern in patterns_to_remove:
        cleaned_report = re.sub(pattern, '', cleaned_report, flags=re.IGNORECASE | re.DOTALL)
    
    # Eliminar l√≠neas iniciales que no sean encabezados (#) si parecen texto de introducci√≥n del LLM
    # Buscar el primer encabezado
    first_header_match = re.search(r'^#\s+', cleaned_report, re.MULTILINE)
    if first_header_match:
        # Si hay texto antes del primer encabezado, verificar si es "basura"
        pre_header = cleaned_report[:first_header_match.start()]
        # Si es corto (< 200 chars) y contiene frases t√≠picas de chat, eliminarlo
        if len(pre_header) < 200 and ("sure" in pre_header.lower() or "here is" in pre_header.lower() or "report" in pre_header.lower()):
            cleaned_report = cleaned_report[first_header_match.start():]
    
    # Limpiar espacios en blanco al inicio y final
    cleaned_report = cleaned_report.strip()
    
    return cleaned_report


def _ensure_references_section(report: str, sources: List[Dict]) -> str:
    """
    Asegura que el reporte tenga una secci√≥n ## References al final.
    Si no existe, la crea autom√°ticamente con todas las fuentes proporcionadas.
    Si existe pero est√° incompleta, agrega las fuentes faltantes.
    """
    import re
    
    # Normalizar URLs para comparaci√≥n (sin trailing slash, lowercase)
    def normalize_url(url: str) -> str:
        if not url or url == 'N/A':
            return ''
        return url.rstrip('/').lower()
    
    # Extraer URLs de las fuentes
    source_urls = {normalize_url(s.get('url', '')): s for s in sources if normalize_url(s.get('url', ''))}
    
    # Verificar si existe secci√≥n ## References (m√°s robusto)
    # Buscamos todas las ocurrencias para consolidarlas
    all_ref_headers = list(re.finditer(r'##\s*References\s*[:\-]*\s*\n?', report, re.IGNORECASE))
    
    if all_ref_headers:
        # Usar la primera ocurrencia como ancla
        first_match = all_ref_headers[0]
        ref_start_pos = first_match.start()
        
        # El "cuerpo" del reporte es todo lo anterior a la primera secci√≥n de referencias
        body_text = report[:ref_start_pos]
        # La secci√≥n de referencias es todo lo posterior (pero limpiaremos otros headers repetidos)
        ref_section_raw = report[first_match.end():]
        
        # Limpiar cualquier otro header "## References" que el LLM haya repetido dentro de la secci√≥n
        ref_section_text = re.sub(r'##\s*References\s*[:\-]*\s*\n?', '', ref_section_raw, flags=re.IGNORECASE)
        ref_section_text = ref_section_text.strip()
        
        # Extraer qu√© n√∫meros de referencia se citan realmente en el cuerpo [1], [2], etc.
        # Buscamos patrones del tipo [1], [1, 2], [1-3]
        cited_nums = set()
        # Patr√≥n para [1], [1, 2], [1,2,3]
        citation_matches = re.findall(r'\[([\d\s,\-]+)\]', body_text)
        for group in citation_matches:
            # Separar por comas
            parts = group.split(',')
            for part in parts:
                part = part.strip()
                if '-' in part:
                    # Rango [1-3]
                    try:
                        start, end = map(int, part.split('-'))
                        cited_nums.update(range(start, end + 1))
                    except: pass
                else:
                    try:
                        cited_nums.add(int(part))
                    except: pass
        
        # Extraer URLs y n√∫meros de las referencias que YA est√°n escritas en la secci√≥n de referencias
        ref_url_pattern = r'https?://[^\s\n]+'
        ref_urls_found = set()
        for match in re.finditer(ref_url_pattern, ref_section_text):
            ref_url = normalize_url(match.group(0).rstrip('.,;)]'))
            ref_urls_found.add(ref_url)
            
        # Filtrar la secci√≥n de referencias existente para eliminar las NO citadas
        ref_lines = ref_section_text.split('\n')
        filtered_ref_lines = []
        for line in ref_lines:
            line = line.strip()
            if not line: continue
            
            # Intentar detectar si es una l√≠nea de referencia [N]
            match = re.match(r'^\s*\[(\d+)\]', line)
            if match:
                ref_num = int(match.group(1))
                if ref_num in cited_nums:
                    filtered_ref_lines.append(line)
            else:
                # Si no empieza por [N], lo mantenemos (podr√≠a ser texto adicional)
                filtered_ref_lines.append(line)
        
        ref_section_text = "\n".join(filtered_ref_lines)
        
        # Identificar fuentes que est√°n CITADAS en el texto pero FALTAN en la lista filtrada
        missing_sources_texts = []
        
        # Mapear fuentes por su √≠ndice 1-based (el que usa el LLM en el prompt)
        for i, source in enumerate(sources, 1):
            url = normalize_url(source.get('url', ''))
            # Si la fuente est√° citada y su URL no est√° en lo que queda de la lista
            if i in cited_nums:
                # Verificar si el URL de la fuente i ya est√° en el texto filtrado
                found_in_filtered = False
                for line in filtered_ref_lines:
                    if url and url in normalize_url(line):
                        found_in_filtered = True
                        break
                
                if not found_in_filtered:
                    title = source.get('title', 'N/A')
                    title = re.sub(r'^\[(PDF|HTML|DOC)\]\s*', '', title)
                    missing_sources_texts.append(f"[{i}] {title} - {source.get('url', 'N/A')}")
        
        # Reconstruir el reporte consolidando la secci√≥n de referencias
        new_report = body_text.rstrip() + "\n\n## References\n\n" + ref_section_text
        if missing_sources_texts:
            print(f"      ‚ö†Ô∏è  {len(missing_sources_texts)} fuente(s) citada(s) pero faltantes en la lista, agreg√°ndolas...")
            new_report = new_report.rstrip() + "\n" + "\n".join(missing_sources_texts) + "\n"
            
        return new_report
    
    # Si no tiene References, agregarla al final (solo las citadas)
    # Si no hay citas detectadas, por seguridad agregamos todas (modo fallback)
    
    # Si no tiene References, agregarla al final
    print(f"      ‚ö†Ô∏è  Secci√≥n ## References no detectada, agreg√°ndola autom√°ticamente...")
    
    # Limpiar el reporte (eliminar espacios finales)
    report = report.rstrip()
    
    # EXTRAER CITAS del texto para el caso fallback
    cited_nums = set()
    citation_matches = re.findall(r'\[([\d\s,\-]+)\]', report)
    for group in citation_matches:
        parts = group.split(',')
        for part in parts:
            part = part.strip()
            if '-' in part:
                try:
                    start, end = map(int, part.split('-'))
                    cited_nums.update(range(start, end + 1))
                except: pass
            else:
                try:
                    cited_nums.add(int(part))
                except: pass

    # Agregar separador si no termina con l√≠nea vac√≠a
    if not report.endswith('\n\n'):
        if not report.endswith('\n'):
            report += '\n'
        report += '\n'
    
    # Agregar secci√≥n References
    report += "## References\n\n"
    
    # Agregar fuentes: si hay citas, solo las citadas. Si no hay citas, todas (fallback total).
    sources_to_add = []
    for i, source in enumerate(sources, 1):
        if not cited_nums or i in cited_nums:
            sources_to_add.append((i, source))
            
    for i, source in sources_to_add:
        title = source.get('title', 'N/A')
        url = source.get('url', 'N/A')
        # Limpiar t√≠tulo
        title = re.sub(r'^\[(PDF|HTML|DOC)\]\s*', '', title)
        report += f"[{i}] {title} - {url}\n"
    
    return report


async def generate_markdown_report(
    topic: str,
    all_sources: List[Dict],
    report_type: str = "General",
    language: str = None,
    reference_style: str = None,
    project_specific_context: Optional[str] = None,
    project_name: Optional[str] = None,
    related_topics: Optional[List[str]] = None,
    hierarchical_context: str = "",
    brief: str = ""
) -> Tuple[str, int]:
    """
    Genera un reporte detallado en Markdown basado en las fuentes.
    Utiliza el LLM Analyst configurado en config.py.
    """
    # Seleccionar el modelo adecuado
    llm = None
    is_test_mode = False
    
    # Deduplicar todas las fuentes por URL al inicio
    unique_sources = []
    seen_urls = set()
    for s in all_sources:
        u = s.get('url', '').rstrip('/').lower()
        if u and u not in seen_urls:
            unique_sources.append(s)
            seen_urls.add(u)
        elif not u:
            unique_sources.append(s)
    all_sources = unique_sources

    if is_test_mode:
        # En modo TEST, usar solo el modelo de TEST (xiaomi/mimo-v2-flash:free)
        llm = llm_analyst
        print(f"      üß™ Modelo: TEST (xiaomi/mimo-v2-flash:free) - Modo TEST activo")
    else:
        # En modo PRODUCTION/ECONOMIC, seleccionar seg√∫n criticidad
        report_types_critical = ["Strategic", "Financial", "Due_Diligence"]
        if report_type in report_types_critical:
            if llm_analyst_precision is not None:
                llm = llm_analyst_precision
                print(f"      üéØ Modelo: Claude Sonnet 4 (Precision) - Reporte CR√çTICO detectado")
            else:
                llm = llm_analyst
                print(f"      ‚ö†Ô∏è  Modelo de precisi√≥n no disponible, usando modelo est√°ndar (Fallback)")
        else:
            if llm_analyst_fast is not None:
                llm = llm_analyst_fast
                print(f"      ‚ö° Modelo: Gemini 2.5 Pro (Fast) - Reporte exploratorio")
            else:
                llm = llm_analyst
                print(f"      ‚ö†Ô∏è  Modelo r√°pido no disponible, usando modelo est√°ndar (Fallback)")
    
    # Verificar si hay contexto del proyecto
    # Verificar tanto None como string vac√≠o
    has_context = project_specific_context and isinstance(project_specific_context, str) and project_specific_context.strip()
    
    if has_context:
        print(f"      ‚úÖ Project context loaded ({len(project_specific_context)} chars). Context used as reference document.")
    else:
        print(f"      ‚ö†Ô∏è  No project-specific context available.")

    if not topic or topic.strip() in ["Sin tema", "N/A", ""]:
        return f"# Error\n\n‚ö†Ô∏è No se puede generar un reporte sin un tema v√°lido.", 0

    # Preparar las fuentes en formato texto para el LLM
    # MEJORA 2026-01: Usar chunking inteligente por relevancia
    # Esto permite incluir fuentes completas priorizadas por score
    # en lugar de truncar uniformemente todas las fuentes

    # Calcular tokens m√°ximos disponibles para fuentes (estimaci√≥n inicial generosa)
    # Se ajustar√° despu√©s si es necesario
    initial_max_tokens_for_sources = 100000  # ~400K chars, se ajustar√° despu√©s

    sources_text, initial_sources_tokens, sources_complete = _chunk_sources_by_relevance(
        sources=all_sources,
        max_total_tokens=initial_max_tokens_for_sources,
        model_name="gpt-4",
        content_field='raw_content',
        fallback_field='snippet'
    )

    print(f"      üìä [CHUNKING] {sources_complete}/{len(all_sources)} fuentes incluidas completas")
    print(f"      üìä [CHUNKING] {initial_sources_tokens:,} tokens iniciales para fuentes")

    # Fallback al m√©todo anterior si el nuevo falla
    if not sources_text:
        content_per_source_limit = 10000
        sources_text = "\n\n".join(
            [
                f"Fuente {i + 1}:\n- T√≠tulo: {s.get('title', 'N/A')}\n- URL: {s.get('url', 'N/A')}\n- Contenido: {s.get('raw_content', s.get('snippet', 'N/A'))[:content_per_source_limit]}"
                for i, s in enumerate(all_sources)
            ]
        )

    # Usar par√°metros pasados o valores por defecto de config
    lang = language or REPORT_LANGUAGE
    ref_style = reference_style or REFERENCES_STYLE

    # Inicializar variables para evitar UnboundLocalError
    context_private_text = ""
    related_topics_text = ""

    # Inyectar instrucciones de estructura al prompt (ya sea custom o fallback)
    base_prompt = (
        reporter_prompt if reporter_prompt else "Act√∫a como un Consultor de Estrategia Senior."
    )

    system_msg = f"""{base_prompt}

INSTRUCCIONES DE ESTRUCTURA Y FORMATO OBLIGATORIAS:
1. IDIOMA: Escribe el reporte √≠ntegramente en {lang}.
2. ESTRUCTURA DEL REPORTE (CR√çTICO - NO OMITIR):
   - Empieza directamente con el t√≠tulo del tema en texto plano: {topic}
   - Sigue con el contenido redactado de forma profesional y sintetizada.
   - üö® OBLIGATORIO: DEBES finalizar el reporte con la secci√≥n de referencias: ## References
   - ‚ùå PROHIBIDO: Finalizar el reporte sin la secci√≥n ## References
   - La secci√≥n ## References es OBLIGATORIA y debe estar al final del documento
   - ‚ùå PROHIBIDO: NO generes secci√≥n "Executive Summary" ni "Resumen Ejecutivo" - esto se generar√° en el documento consolidado final
3. CITAS EN EL TEXTO (ESTILO PROFESIONAL OBLIGATORIO):
   - Estilo: {ref_style} (IEEE: [1], [2]...).
   - ORDEN: Las citas deben numerarse consecutivamente en orden de aparici√≥n ([1], luego [2], etc.).
   - REGLA CR√çTICA: CADA DATO ESPEC√çFICO (n√∫meros, estad√≠sticas, porcentajes, fechas, nombres propios, cifras) 
     DEBE tener su cita correspondiente al final del p√°rrafo donde aparece.
   - REGLAS DE ESTILO (VER ABAJO SECCI√ìN DETALLADA DE EJEMPLOS).
   
4. SECCI√ìN DE REFERENCIAS (## References):
   - OBLIGATORIO: Debe incluir TODAS las fuentes que se citaron en el texto.
   - Formato por cada fuente: [N√∫mero] T√≠tulo de la fuente - URL
   - Ejemplo correcto: [1] Market Analysis Report 2024 - https://example.com/report
   - Ejemplo correcto: [2] Industry Trends and Growth Projections - https://example.com/trends
   - Las referencias deben numerarse consecutivamente [1], [2], [3]... seg√∫n orden de primera aparici√≥n en el texto
   - Formato exacto requerido: [N] T√≠tulo - URL (con gui√≥n " - " separando t√≠tulo y URL)
   - ‚ùå INCORRECTO: [1] URL (falta t√≠tulo)
   - ‚ùå INCORRECTO: [1] T√≠tulo URL (falta separador)
   - ‚ùå INCORRECTO: T√≠tulo - URL (falta [N√∫mero])

5. REGLAS DE ORO:

   **üéØ GU√çA DETALLADA DE ESTILO PROFESIONAL PARA CITAS (OBLIGATORIO):**
   
   ‚ùå ESTILO INCORRECTO (Excesivo, poco profesional):
   ```
   The company is a leader [1][2][3]. The organization has strong growth [4][5].
   Bank of America recognizes this [1][2]. The pricing power is evident [1][2][6].
   ```
   
   ‚úÖ ESTILO CORRECTO (Profesional, acad√©mico):
   ```
   The company is a global leader in its sector, strategically 
   positioned for significant growth by 2026. Market analysts have recognized 
   the company as a top pick, driven by strong pricing power in its regional 
   portfolio, which is expected to achieve significant growth through 2029 [1, 2].
   ```
   
   **PRINCIPIOS NO NEGOCIABLES:**
   
   1. **CITA AL FINAL DEL P√ÅRRAFO (OBLIGATORIO PARA DATOS ESPEC√çFICOS):**
      - Cada dato espec√≠fico mencionado (porcentajes, cifras, fechas, estad√≠sticas, nombres propios, montos) 
        DEBE tener su cita correspondiente al final del p√°rrafo.
      - Si un p√°rrafo contiene m√∫ltiples datos espec√≠ficos de diferentes fuentes, agrupa las citas: [1, 2, 3] al final.
      - Si todos los datos del p√°rrafo vienen de la misma fuente, usa una sola cita: [1].
      - ‚ùå NO cites frase por frase, pero S√ç cita cada p√°rrafo que contenga datos espec√≠ficos.
   
   2. **AGRUPA REFERENCIAS:**
      - Usa [1, 2, 3] con comas al final del p√°rrafo.
      - ‚ùå NUNCA uses [1][2][3].
      - M√ÅXIMO 3 referencias por grupo: [1, 2, 3]. Si hay m√°s datos, elige las 3 fuentes m√°s relevantes.
      - Si tienes 4+ fuentes con datos relevantes, prioriza las m√°s confiables y agrupa: [1, 2, 3].
   
   3. **FRECUENCIA OBLIGATORIA:**
      - Cada p√°rrafo que contenga datos espec√≠ficos DEBE tener al menos 1 cita al final.
      - Si un p√°rrafo tiene solo texto descriptivo sin datos concretos, puede no llevar cita (caso excepcional).
      - P√°rrafos con datos num√©ricos, estad√≠sticas o informaci√≥n verificable: SIEMPRE citar.
   
   4. **PRIORIDAD DE FUENTES:**
      - Cita fuentes primarias (informes oficiales) antes que noticias secundarias.
      - Si tienes el mismo dato en m√∫ltiples fuentes, cita la m√°s confiable primero.

5. REGLAS FUNDAMENTALES:
   - No resumas fuente por fuente; agrupa por temas.
   - S√© directo y ejecutivo.
   - No te inventes nuevos cap√≠tulos, es decir, no crees nuevos t√≠tulos con # o ## o del estilo.
   - Usa SOLAMENTE la informaci√≥n de las fuentes proporcionadas.
   - No crees nuevos t√≠tulos dentro del reporte con # o ## o del estilo. Todo tiene que estar redactado.
   - Si quieres crear apartados, usa el s√≠mbolo * para crearlo en negrita, pero NUNCA crees nuevos t√≠tulos.
   - No utilices bullet points a menos que estos sean necesarios para la redacci√≥n. La redacci√≥n debe ser extensa y detallada.

8. üö´ PROHIBIDO INCLUIR METADATOS O PROCESO DE GENERACI√ìN (CR√çTICO):
   - ‚ùå PROHIBIDO incluir mensajes sobre tu proceso de trabajo como:
     * "Drafting the Report", "Finalizing the Report", "I'm now drafting", "I'm starting with", "I'm reviewing"
     * "I'm now finalizing", "I'm confident that", "I've completed the draft"
     * Cualquier texto que describa lo que est√°s haciendo o pensando
   - ‚ùå PROHIBIDO incluir Confidence Scores, badges, o m√©tricas de confianza en el contenido del reporte
   - ‚ùå PROHIBIDO incluir mensajes como "üü¢ Confidence Score: 100/100" o similares
   - ‚ùå PROHIBIDO incluir cualquier texto que no sea el contenido real del reporte
   - ‚úÖ SOLO incluye el contenido del reporte: t√≠tulo, texto redactado, y secci√≥n de referencias
   - ‚úÖ Empieza directamente con el t√≠tulo del tema y el contenido, sin pre√°mbulos ni metadatos
   - ‚úÖ El reporte debe ser el contenido final, no una descripci√≥n del proceso de generaci√≥n

IMPORTANTE!!:
   - SOLAMENTE tiene que tener HASHTAGS # el t√≠tulo del tema y ## la secci√≥n de referencias.
   - No crees nuevos t√≠tulos dentro del reporte con HASHTAGS: # o ##. Todo tiene que estar redactado.
   - No incluyas la informaci√≥n privada a menos que el t√≠tulo del reporte se refiera expl√≠citamente a la empresa cliente.
   - NO incluyas metadatos, mensajes de proceso, o cualquier texto que no sea el contenido real del reporte.

6. ALINEACI√ìN TOTAL TEMA-PROYECTO DE INVESTIGACI√ìN:
   - Todo lo que escribas sobre '{topic}' debe responder a una pregunta: ¬øC√≥mo contribuye esto al objetivo del proyecto '{project_name}'?
   - No escribas gen√©ricamente sobre el tema. Escribe sobre el tema EN EL CONTEXTO del proyecto.
   - Si el proyecto es "Estrategia 2030" y el tema es "IA", no hables de la historia de la IA, habla de "Impacto de la IA en la Estrategia 2030".

7. REGLAS ANTI-ALUCINACI√ìN (CR√çTICO - PRIORIDAD ABSOLUTA):
   
   üö® PROHIBIDO INVENTAR (ZERO TOLERANCE):
   - NUNCA inventes estad√≠sticas, porcentajes, cifras de mercado o proyecciones num√©ricas
   - NUNCA inventes nombres de empresas, personas, productos o marcas que no aparezcan en las fuentes
   - NUNCA inventes fechas, a√±os, plazos temporales o cronolog√≠as
   - NUNCA extrapoles tendencias m√°s all√° de lo que dicen expl√≠citamente las fuentes
   - NUNCA hagas inferencias num√©ricas o comparativas sin datos expl√≠citos
   - NUNCA asumas datos de contexto general si no est√°n en las fuentes proporcionadas
   - NUNCA uses conocimiento previo para "completar" informaci√≥n faltante
   
   ‚úÖ SI NO TIENES DATOS (PROTOCOLO OBLIGATORIO):
   - Usa EXACTAMENTE estos formatos cuando falte informaci√≥n:
     * "Seg√∫n las fuentes consultadas, no se dispone de informaci√≥n espec√≠fica sobre..."
     * "Los datos disponibles no permiten cuantificar..."
     * "Las fuentes no especifican..."
     * "La informaci√≥n proporcionada no incluye datos sobre..."
   - Si una fuente menciona algo de forma vaga, repite la vaguedad, NO la conviertas en algo espec√≠fico
   
   üìä DATOS NUM√âRICOS (VERIFICACI√ìN OBLIGATORIA):
   - TODO dato num√©rico (porcentajes, cifras, a√±os, montos) DEBE tener una cita espec√≠fica [X] al final del p√°rrafo
   - REGLA DE ORO: Si mencionas un n√∫mero, porcentaje, fecha o cifra ‚Üí SIEMPRE cita al final del p√°rrafo [X]
   - Ejemplo correcto: "El mercado creci√≥ un 25% en 2024, alcanzando 500 millones de euros [1]."
   - Si una fuente dice "crecimiento significativo" sin cifras, escribe "crecimiento significativo" - NUNCA a√±adas "del 15%" o similar
   - Reproduce rangos EXACTOS: si la fuente dice "15-20%", escribe "15-20%", NO "aproximadamente 17%" o "alrededor del 18%"
   - Si la fuente dice "millones" sin especificar, escribe "millones", NO inventes "5 millones" o "varios millones"
   - Si hay m√∫ltiples fuentes con datos diferentes, cita ambas al final: "Seg√∫n las fuentes, los datos var√≠an entre X e Y [1, 2]. En caso de duda y si viene de la misma fuente, elige la cifra mas actualizada en base a la fuente"
   
   üîç VERIFICACI√ìN INTERNA OBLIGATORIA (PROCESO PASO A PASO):
   ANTES de escribir CADA p√°rrafo o afirmaci√≥n:
   1. Preg√∫ntate: "¬øEn qu√© fuente espec√≠fica (n√∫mero) est√° esta informaci√≥n?"
   2. Si puedes identificar la fuente: incl√∫yela con la cita [X] correspondiente
   3. Si NO puedes identificar la fuente exacta: NO la incluyas bajo ning√∫n concepto
   4. Si la informaci√≥n est√° "impl√≠cita" o "sugerida" pero no expl√≠cita: NO la incluyas
   5. Si est√°s "seguro de que es verdad" pero no est√° en las fuentes: NO la incluyas
   
   ‚ö†Ô∏è CASOS ESPECIALES:
   - Informaci√≥n contradictoria: "Las fuentes presentan informaci√≥n divergente: [1] indica X, mientras que [2] indica Y"
   - Informaci√≥n parcial: "Seg√∫n [1], se menciona X, aunque no se proporcionan detalles adicionales sobre Y"
   - Inferencias prohibidas: Si las fuentes dicen "A y B", NO escribas "A, B y por lo tanto C" a menos que C est√© expl√≠cito
   
   ‚úÖ EJEMPLO CORRECTO (CITAS AL FINAL DEL P√ÅRRAFO):
   "El mercado ha experimentado un crecimiento del 25% en el √∫ltimo a√±o, alcanzando 
   los 500 millones de euros en facturaci√≥n. Este incremento se debe principalmente 
   a factores tecnol√≥gicos identificados en m√∫ltiples an√°lisis sectoriales. 
   Seg√∫n proyecciones recientes, se espera que esta tendencia contin√∫e hasta 2030 
   con un crecimiento anual estimado del 30% [1, 2, 3]."
   
   "La empresa l√≠der del sector report√≥ ventas de 1.200 millones en 2024, 
   representando un incremento del 18% respecto al a√±o anterior [4]."
   
   ‚ùå EJEMPLO INCORRECTO (SIN CITAS O ALUCINACI√ìN):
   "El mercado ha experimentado un crecimiento del 25% seg√∫n m√∫ltiples analistas. 
   Este incremento se debe principalmente a factores tecnol√≥gicos y se espera que 
   contin√∫e hasta 2030 con una proyecci√≥n del 30% anual."
   
   (Problemas: 
   - No tiene cita [X] al final del p√°rrafo a pesar de mencionar "25%", "2030", "30%"
   - Si estos datos son reales de las fuentes, DEBEN tener cita: [1, 2, 3]
   - Si son inventados, viola reglas anti-alucinaci√≥n)
   
   ‚ùå EJEMPLO INCORRECTO (CITA EN LUGAR INCORRECTO):
   "El mercado creci√≥ [1] un 25% el a√±o pasado, alcanzando 500 millones [2]."
   
   (Problema: citas intercaladas en el texto, deben ir al final del p√°rrafo: 
   "El mercado creci√≥ un 25% el a√±o pasado, alcanzando 500 millones [1, 2].")
"""


    context_text = f"Nombre del Proyecto: {project_name}\n" if project_name else ""

    # NOTA: El contexto de la empresa ahora viene de Airtable (campo Context en Proyectos)
    # No se usa company_context del JSON, se usa project_specific_context de Airtable
    # company_context ya no se usa - toda la informaci√≥n viene en project_specific_context

    # Usar la misma verificaci√≥n que arriba
    if has_context:
        context_private_text += "--- INFORMACI√ìN PRIVADA ---\n"
        context_private_text += f"{project_specific_context}\n"
        context_private_text += f"""
    INSTRUCCI√ìN CR√çTICA SOBRE CONTEXTO INTERNO (SEGURIDAD Y PERTINENCIA): 
    
    1. üõ°Ô∏è PROHIBICI√ìN DE DATOS SENSIBLES:
       - NUNCA reveles cifras financieras internas confidenciales (EBITDA, m√°rgenes detallados, proyecciones no p√∫blicas) que aparezcan en los documentos privados.
       - Si el documento privado habla de estrategias futuras confidenciales, √∫salo solo para entender el contexto, NO para revelarlas textualmente.

    2. üéØ USO SELECTIVO SEG√öN EL TEMA DEL CAP√çTULO (Gold Rule):
       - Analiza el T√çTULO del tema actual: "{topic}"
       - SI el tema es sobre Mercado General, Tendencias, Competencia o Tecnolog√≠a (Outside-In):
         -> ¬°IGNORA EL CONTEXTO PRIVADO! 
         -> NO menciones el nombre de la empresa cliente. 
         -> C√©ntrate 100% en la investigaci√≥n externa (URLs).
       - SI (y SOLO SI) el tema pide expl√≠citamente "Implicaciones para [Empresa]", "Gap Analysis", "Comparativa" o "Oportunidades para [Empresa]":
         -> Usa el contexto privado para contrastar.
         -> Aterriza los hallazgos externos a la realidad de la empresa.
    
    3. ‚öñÔ∏è FILOSOF√çA DE REDACCI√ìN:
       - Tu "Norte" es el T√çTULO DEL PROYECTO. Todo lo que escribas debe aportar valor a ese objetivo.
       - El contexto privado es tu "conocimiento t√°cito" para saber qu√© es relevante, no tu fuente de contenido para "copiar y pegar".
    
    4. üö´ CERO REFERENCIAS PRIVADAS:
       - PROHIBIDO incluir documentos del "Contexto Privado" en la secci√≥n ## References.
       - PROHIBIDO citar con n√∫mero [X] informaci√≥n que viene de los documentos internos.
       - Las referencias [X] son EXCLUSIVAMENTE para fuentes p√∫blicas (URLs).
    \n"""

    if related_topics:
        # El usuario ya incluy√≥ el header en user_msg, solo agregamos los items a la variable
        for rt in related_topics:
            related_topics_text += f"- {rt}\n"

    user_msg = f""" 
Contexto del proyecto: {context_text} 

Tema actual a redactar: {topic}
{"" if not hierarchical_context else chr(10) + hierarchical_context + chr(10)}
{"" if not brief else "BRIEF/OBJETIVO DEL CAP√çTULO:" + chr(10) + brief + chr(10)}
Otros temas relacionados en este mismo proyecto (EVITAR SOLAPAMIENTOS O REPETICIONES):
{related_topics_text}

Lista de Fuentes a utilizar SIEMPRE:
{sources_text}

Informaci√≥n privada (solo si el tema EXPLICITAMENTE lo requiere):
{context_private_text}

üö® RECORDATORIO FINAL OBLIGATORIO:
- El reporte DEBE terminar con la secci√≥n ## References
- DEBES incluir TODAS las fuentes listadas arriba en la secci√≥n ## References
- Formato: [N√∫mero] T√≠tulo - URL para cada fuente
- NO omitas la secci√≥n ## References bajo ning√∫n concepto

Genera el reporte siguiendo las reglas de formato especificadas."""

    try:
        # Verificar que el LLM est√© inicializado
        if llm is None:
            raise ValueError("LLM no est√° inicializado. Verifica la configuraci√≥n de los modelos en config.py")
        
        # ============================================
        # TRUNCAMIENTO DE TOKENS ANTES DE ENVIAR
        # ============================================
        from .utils import count_tokens
        
        # Obtener nombre del modelo y determinar l√≠mite de tokens
        model_name = ""
        provider = None
        
        try:
            if hasattr(llm, 'model_name'):
                model_name = llm.model_name
            elif hasattr(llm, 'model'):
                model_name = llm.model
            elif hasattr(llm, '_default_params') and 'model' in llm._default_params:
                model_name = llm._default_params['model']
                
            # Detectar proveedor
            if USE_DEEPSEEK_FOR_TESTING:
                roles_key = "roles_test"
            elif USE_CHEAP_OPENROUTER_MODELS:
                roles_key = "roles_cheap"
            else:
                roles_key = "roles"
            
            roles_config = TOML_CONFIG.get(roles_key, {})
            analyst_config = roles_config.get("analyst", {})
            provider = analyst_config.get("provider")
        except Exception:
            pass

        # Usar l√≥gica centralizada
        MAX_TOKENS_MODEL, MAX_TOKENS_AVAILABLE = get_model_limits(model_name, provider)
        
        # Detectar si es DeepInfra (solo para l√≥gica compleja de truncamiento m√°s abajo)
        is_deepinfra = provider == "openrouter" and ("deepseek" in (model_name or "").lower() or "mimo" in (model_name or "").lower())
        
        # Calcular tokens del prompt completo
        system_tokens = count_tokens(system_msg, model_name or "gpt-4")
        user_msg_tokens = count_tokens(user_msg, model_name or "gpt-4")
        total_tokens = system_tokens + user_msg_tokens
        
        print(f"      üìä Tokens calculados: {total_tokens:,} (system: {system_tokens:,}, user: {user_msg_tokens:,})")
        print(f"      üìä L√≠mite del modelo: {MAX_TOKENS_MODEL:,} tokens (disponible para input: {MAX_TOKENS_AVAILABLE:,})")
        
        # Si excede el l√≠mite, truncar contenido
        if total_tokens > MAX_TOKENS_AVAILABLE:
            print(f"      ‚ö†Ô∏è  El prompt excede el l√≠mite. Truncando contenido...")
            print(f"      üìä Tokens estimados: {total_tokens:,} (l√≠mite: {MAX_TOKENS_AVAILABLE:,})")
            
            # Calcular tokens disponibles para contenido (despu√©s de system message y template)
            # El template incluye: context_text, topic, related_topics_text, y las instrucciones finales
            template_text = f""" 
Contexto del proyecto: {context_text} 

Tema actual a redactar: {topic}
{"" if not hierarchical_context else chr(10) + hierarchical_context + chr(10)}
{"" if not brief else "BRIEF/OBJETIVO DEL CAP√çTULO:" + chr(10) + brief + chr(10)}
Otros temas relacionados en este mismo proyecto (EVITAR SOLAPAMIENTOS O REPETICIONES):
{related_topics_text}

Lista de Fuentes a utilizar SIEMPRE:
{{sources_text}}

Informaci√≥n privada (solo si el tema EXPLICITAMENTE lo requiere):
{{context_private_text}}

üö® RECORDATORIO FINAL OBLIGATORIO:
- El reporte DEBE terminar con la secci√≥n ## References
- DEBES incluir TODAS las fuentes listadas arriba en la secci√≥n ## References
- Formato: [N√∫mero] T√≠tulo - URL para cada fuente
- NO omitas la secci√≥n ## References bajo ning√∫n concepto

Genera el reporte siguiendo las reglas de formato especificadas."""
            
            template_tokens = count_tokens(template_text, model_name or "gpt-4")
            # Para DeepInfra, usar margen de seguridad m√°s conservador (80% en lugar de 85%)
            safety_margin = 0.80 if is_deepinfra else 0.95
            tokens_for_content = max(5000 if is_deepinfra else 10000, int((MAX_TOKENS_AVAILABLE - system_tokens - template_tokens) * safety_margin))
            
            # Distribuir tokens: ajustar seg√∫n si es DeepInfra o no
            if is_deepinfra:
                # DeepInfra: 60% para fuentes, 40% para contexto privado (m√°s balanceado)
                tokens_for_sources = int(tokens_for_content * 0.6)
                tokens_for_context = tokens_for_content - tokens_for_sources
            else:
                # Otros: 70% para fuentes, 30% para contexto privado
                tokens_for_sources = int(tokens_for_content * 0.7)
                tokens_for_context = tokens_for_content - tokens_for_sources
            
            # Truncar fuentes usando chunking inteligente
            sources_tokens = count_tokens(sources_text, model_name or "gpt-4")
            if sources_tokens > tokens_for_sources:
                print(f"      üîß Re-chunking fuentes: {sources_tokens:,} -> {tokens_for_sources:,} tokens")
                # MEJORA: Usar chunking inteligente en lugar de truncamiento uniforme
                sources_text, sources_tokens, sources_complete = _chunk_sources_by_relevance(
                    sources=all_sources,
                    max_total_tokens=tokens_for_sources,
                    model_name=model_name or "gpt-4"
                )
                print(f"      ‚úÖ Fuentes re-chunked: {sources_tokens:,} tokens ({sources_complete}/{len(all_sources)} completas)")
            
            # Truncar contexto privado
            context_tokens = count_tokens(context_private_text, model_name or "gpt-4")
            if context_tokens > tokens_for_context:
                print(f"      üîß Truncando contexto privado: {context_tokens:,} -> {tokens_for_context:,} tokens")
                chars_per_token = len(context_private_text) / context_tokens if context_tokens > 0 else 4
                max_chars_context = int(tokens_for_context * chars_per_token)
                context_private_text = context_private_text[:max_chars_context] + "\n\n[... contexto truncado por l√≠mite de tokens ...]"
                context_tokens = count_tokens(context_private_text, model_name or "gpt-4")
                print(f"      ‚úÖ Contexto truncado: {context_tokens:,} tokens")
            
            # Reconstruir user_msg con contenido truncado
            user_msg = f""" 
Contexto del proyecto: {context_text} 

Tema actual a redactar: {topic}
{"" if not hierarchical_context else chr(10) + hierarchical_context + chr(10)}
{"" if not brief else "BRIEF/OBJETIVO DEL CAP√çTULO:" + chr(10) + brief + chr(10)}
Otros temas relacionados en este mismo proyecto (EVITAR SOLAPAMIENTOS O REPETICIONES):
{related_topics_text}

Lista de Fuentes a utilizar SIEMPRE:
{sources_text}

Informaci√≥n privada (solo si el tema EXPLICITAMENTE lo requiere):
{context_private_text}

üö® RECORDATORIO FINAL OBLIGATORIO:
- El reporte DEBE terminar con la secci√≥n ## References
- DEBES incluir TODAS las fuentes listadas arriba en la secci√≥n ## References
- Formato: [N√∫mero] T√≠tulo - URL para cada fuente
- NO omitas la secci√≥n ## References bajo ning√∫n concepto

Genera el reporte siguiendo las reglas de formato especificadas."""
            
            # Verificar tokens finales
            final_tokens = system_tokens + count_tokens(user_msg, model_name or "gpt-4")
            
            # CALCULAR P√âRDIDA DE CALIDAD (WARNING)
            original_content_tokens = sources_tokens + context_tokens
            final_content_tokens = count_tokens(sources_text, model_name or "gpt-4") + count_tokens(context_private_text, model_name or "gpt-4")
            
            if original_content_tokens > 0:
                retention_rate = (final_content_tokens / original_content_tokens) * 100
                if retention_rate < 50:
                    print(f"      üö® ADVERTENCIA DE CALIDAD: Solo se ha retenido el {retention_rate:.1f}% de la informaci√≥n original.")
                    print(f"         Esto puede afectar significativamente la profundidad del reporte.")
                    print(f"         Sugerencia: Cambiar rol 'analyst' a un modelo con mayor contexto (ej: gemini-2.0-flash).")
            
            # Verificaci√≥n final: asegurar que no exceda el l√≠mite
            max_allowed = MAX_TOKENS_MODEL if is_deepinfra else MAX_TOKENS_AVAILABLE
            if final_tokens > max_allowed:
                provider_name = "DeepInfra" if is_deepinfra else "el proveedor"
                print(f"      ‚ö†Ô∏è  {provider_name}: prompt a√∫n excede l√≠mite ({final_tokens:,} > {max_allowed:,}), aplicando truncamiento m√°s agresivo...")
                # Reducir a√∫n m√°s el contenido
                if is_deepinfra:
                    # DeepInfra: usar solo 60% del l√≠mite total para el prompt (muy agresivo)
                    MAX_PROMPT_TOKENS_DEEPINFRA = int(MAX_TOKENS_MODEL * 0.60)
                    tokens_for_content = max(3000, int((MAX_PROMPT_TOKENS_DEEPINFRA - system_tokens - template_tokens) * 0.90))
                else:
                    # Otros: usar 85% del l√≠mite disponible
                    tokens_for_content = max(5000, int((MAX_TOKENS_AVAILABLE - system_tokens - template_tokens) * 0.85))
                tokens_for_sources = int(tokens_for_content * 0.6)
                tokens_for_context = tokens_for_content - tokens_for_sources
                
                # Re-truncar con nuevos l√≠mites m√°s restrictivos usando chunking inteligente
                if sources_tokens > tokens_for_sources:
                    sources_text, sources_tokens, sources_complete = _chunk_sources_by_relevance(
                        sources=all_sources,
                        max_total_tokens=tokens_for_sources,
                        model_name=model_name or "gpt-4"
                    )
                    print(f"      üîß Re-chunking agresivo: {sources_complete}/{len(all_sources)} fuentes completas")
                
                if context_tokens > tokens_for_context:
                    chars_per_token = len(context_private_text) / context_tokens if context_tokens > 0 else 4
                    max_chars_context = int(tokens_for_context * chars_per_token)
                    context_private_text = context_private_text[:max_chars_context] + "\n\n[... contexto truncado por l√≠mite de tokens ...]"
                    context_tokens = count_tokens(context_private_text, model_name or "gpt-4")
                
                # Reconstruir user_msg
                user_msg = f""" 
Contexto del proyecto: {context_text} 

Tema actual a redactar: {topic}
{"" if not hierarchical_context else chr(10) + hierarchical_context + chr(10)}
{"" if not brief else "BRIEF/OBJETIVO DEL CAP√çTULO:" + chr(10) + brief + chr(10)}
Otros temas relacionados en este mismo proyecto (EVITAR SOLAPAMIENTOS O REPETICIONES):
{related_topics_text}

Lista de Fuentes a utilizar SIEMPRE:
{sources_text}

Informaci√≥n privada (solo si el tema EXPLICITAMENTE lo requiere):
{context_private_text}

üö® RECORDATORIO FINAL OBLIGATORIO:
- El reporte DEBE terminar con la secci√≥n ## References
- DEBES incluir TODAS las fuentes listadas arriba en la secci√≥n ## References
- Formato: [N√∫mero] T√≠tulo - URL para cada fuente
- NO omitas la secci√≥n ## References bajo ning√∫n concepto

Genera el reporte siguiendo las reglas de formato especificadas."""
                
                final_tokens = system_tokens + count_tokens(user_msg, model_name or "gpt-4")
                print(f"      ‚ö†Ô∏è  Re-truncamiento agresivo aplicado: {final_tokens:,} tokens finales")
                
                # Verificaci√≥n final: si a√∫n excede, aplicar truncamiento extremo
                max_allowed = MAX_TOKENS_MODEL if is_deepinfra else MAX_TOKENS_AVAILABLE
                if final_tokens > max_allowed:
                    provider_name = "DeepInfra" if is_deepinfra else "el proveedor"
                    print(f"      ‚ö†Ô∏è  Truncamiento extremo necesario: {final_tokens:,} > {max_allowed:,}")
                    # Truncamiento extremo: reducir fuentes a m√≠nimo absoluto
                    if is_deepinfra:
                        # DeepInfra: usar solo 50% del l√≠mite total
                        MAX_PROMPT_TOKENS_EXTREME = int(MAX_TOKENS_MODEL * 0.20)  # Muy agresivo para DeepSeek 32K
                        tokens_for_content = max(2000, int((MAX_PROMPT_TOKENS_EXTREME - system_tokens - template_tokens) * 0.85))
                    else:
                        tokens_for_content = max(3000, int((MAX_TOKENS_AVAILABLE - system_tokens - template_tokens) * 0.75))
                    
                    tokens_for_sources = int(tokens_for_content * 0.6)
                    tokens_for_context = tokens_for_content - tokens_for_sources
                    
                    # Truncar fuentes extremadamente - usar chunking inteligente
                    if sources_tokens > tokens_for_sources:
                        sources_text, sources_tokens, sources_complete = _chunk_sources_by_relevance(
                            sources=all_sources,
                            max_total_tokens=tokens_for_sources,
                            model_name=model_name or "gpt-4"
                        )
                        print(f"      üö® Truncamiento extremo: solo {sources_complete}/{len(all_sources)} fuentes (priorizadas por relevancia)")
                    
                    # Truncar contexto extremadamente
                    if context_tokens > tokens_for_context:
                        chars_per_token = len(context_private_text) / context_tokens if context_tokens > 0 else 4
                        max_chars_context = int(tokens_for_context * chars_per_token)
                        context_private_text = context_private_text[:max_chars_context] + "\n\n[... contexto truncado por l√≠mite de tokens ...]"
                        context_tokens = count_tokens(context_private_text, model_name or "gpt-4")
                    
                    # Reconstruir user_msg
                    user_msg = f""" 
Contexto del proyecto: {context_text} 

Tema actual a redactar: {topic}
{"" if not hierarchical_context else chr(10) + hierarchical_context + chr(10)}
{"" if not brief else "BRIEF/OBJETIVO DEL CAP√çTULO:" + chr(10) + brief + chr(10)}
Otros temas relacionados en este mismo proyecto (EVITAR SOLAPAMIENTOS O REPETICIONES):
{related_topics_text}

Lista de Fuentes a utilizar SIEMPRE:
{sources_text}

Informaci√≥n privada (solo si el tema EXPLICITAMENTE lo requiere):
{context_private_text}

üö® RECORDATORIO FINAL OBLIGATORIO:
- El reporte DEBE terminar con la secci√≥n ## References
- DEBES incluir TODAS las fuentes listadas arriba en la secci√≥n ## References
- Formato: [N√∫mero] T√≠tulo - URL para cada fuente
- NO omitas la secci√≥n ## References bajo ning√∫n concepto

Genera el reporte siguiendo las reglas de formato especificadas."""
                    
                    final_tokens = system_tokens + count_tokens(user_msg, model_name or "gpt-4")
                    print(f"      ‚ö†Ô∏è  Truncamiento extremo aplicado: {final_tokens:,} tokens finales")
            
            # Verificaci√≥n final antes de enviar
            max_allowed = MAX_TOKENS_MODEL if is_deepinfra else MAX_TOKENS_AVAILABLE
            if final_tokens > max_allowed:
                provider_name = "DeepInfra" if is_deepinfra else "el proveedor"
                print(f"      ‚ùå ERROR: No se puede reducir el prompt a menos de {max_allowed:,} tokens (actual: {final_tokens:,})")
                print(f"      ‚ö†Ô∏è  El contenido es demasiado grande. Se usar√° el fallback.")
                raise ValueError(f"Prompt demasiado grande para {provider_name}: {final_tokens:,} tokens > {max_allowed:,} tokens")
            
            print(f"      ‚úÖ Truncamiento completado: {final_tokens:,} tokens finales (dentro del l√≠mite de {max_allowed:,})")
        else:
            # No necesita truncamiento, calcular tokens finales
            final_tokens = total_tokens
        
        print(f"      üìä Enviando {len(sources_text)} caracteres al modelo...")
        print(f"      üîç Modelo utilizado: {type(llm).__name__} ({model_name if model_name else 'desconocido'})")
        
        response = await llm.ainvoke(
            [{"role": "system", "content": system_msg}, {"role": "user", "content": user_msg}]
        )

        report = response.content if hasattr(response, "content") else str(response)
        
        # Limpiar metadatos y mensajes de proceso del reporte
        report = _clean_report_metadata(report)
        
        # Verificar que el reporte no est√© vac√≠o
        if not report or len(report.strip()) < 100:
            raise ValueError(f"El LLM devolvi√≥ un reporte vac√≠o o muy corto ({len(report) if report else 0} caracteres)")
        
        # Auto-correcci√≥n: Agregar secci√≥n ## References si falta
        report = _ensure_references_section(report, all_sources)

        from .utils import count_tokens
        full_prompt_text = system_msg + "\n" + user_msg
        tokens = count_tokens(full_prompt_text)

        print(f"      ‚úÖ Reporte generado exitosamente ({len(report)} caracteres, ~{tokens} tokens)")
        return report, tokens

    except Exception as e:
        import traceback
        error_details = traceback.format_exc()
        print(f"      ‚ùå Error al generar reporte con el LLM:")
        print(f"      üî¥ Tipo de error: {type(e).__name__}")
        print(f"      üî¥ Mensaje: {str(e)}")
        print(f"      üî¥ Traceback completo:")
        print(f"      {error_details}")
        
        # Fallback: reporte simple con contenido real basado en fuentes
        print(f"      ‚ö†Ô∏è Generando reporte simple como fallback...")
        report = f"# {topic}\n\n"
        
        # Generar contenido b√°sico basado en las fuentes
        if all_sources:
            report += f"Este reporte analiza {topic} bas√°ndose en {len(all_sources)} fuentes de informaci√≥n. "
            report += "A continuaci√≥n se presenta un resumen de los hallazgos principales.\n\n"
            
            # Agrupar fuentes por tema o extraer informaci√≥n clave
            # Tomar las primeras 5-10 fuentes m√°s relevantes y extraer snippets
            top_sources = sorted(all_sources, key=lambda x: x.get('total_score', x.get('score', 0)), reverse=True)[:10]
            
            report += "## Hallazgos Principales\n\n"
            
            for i, source in enumerate(top_sources, 1):
                title = source.get('title', 'N/A')
                snippet = source.get('snippet', source.get('raw_content', ''))[:500]  # Primeros 500 caracteres
                url = source.get('url', 'N/A')
                
                if snippet and snippet.strip():
                    report += f"Seg√∫n {title} [{i}], {snippet.strip()}\n\n"
                else:
                    report += f"La fuente {title} [{i}] proporciona informaci√≥n relevante sobre {topic}.\n\n"
            
            # Si hay m√°s fuentes, mencionarlas brevemente
            if len(all_sources) > len(top_sources):
                report += f"\nAdicionalmente, se consultaron {len(all_sources) - len(top_sources)} fuentes adicionales que proporcionan informaci√≥n complementaria sobre {topic}.\n\n"
        else:
            report += f"No se encontraron fuentes suficientes para generar un reporte completo sobre {topic}.\n\n"
        
        # CR√çTICO: Agregar secci√≥n ## References al fallback tambi√©n
        report = _ensure_references_section(report, all_sources)
        
        # Calcular tokens del fallback tambi√©n
        from .utils import count_tokens
        tokens = count_tokens(report)
        return report, tokens


async def generate_final_report(topic: str, knowledge_base: List[Dict]) -> str:
    """
    Genera un reporte final detallado usando Gemini 2.5 Pro para s√≠ntesis masiva.
    NOTE: This synchronous version is kept for compatibility if needed, 
    but for the main pipeline we should probably use an async version or wrap it.
    If this is not used in the main async pipeline, we can leave it as is.
    However, if it IS used, it should be async. Assuming it is NOT used in the graph loop based on graph.py.
    """
    # ... implementation stays sync unless verified it's used in async path ...
    # Actually, let's check graph.py. Only generate_markdown_report is imported and used in reporter_node.
    # So we can leave this one as is or make it async if we want consistency.
    # Let's leave it for now to avoid breaking other scripts unless specified.
    return "Legacy function - use generate_markdown_report"
    print(f"\n   üß† [GEMINI ANALYST] Generando s√≠ntesis final con Gemini 2.5 Pro...")
    print(f"      Procesando {len(knowledge_base)} fuentes validadas...")

    if not knowledge_base:
        return "‚ö†Ô∏è No se encontraron fuentes v√°lidas para este tema."

    # Construir el contenido completo de la base de conocimientos
    # Gemini puede manejar contextos enormes, as√≠ que incluimos todo
    knowledge_content = f"TEMA DE INVESTIGACI√ìN: {topic}\n\n"
    knowledge_content += "=" * 80 + "\n"
    knowledge_content += "BASE DE CONOCIMIENTOS RECOPILADA\n"
    knowledge_content += "=" * 80 + "\n\n"

    for i, source in enumerate(knowledge_base, 1):
        knowledge_content += f"--- FUENTE {i} ---\n"
        knowledge_content += f"T√≠tulo: {source.get('title', 'N/A')}\n"
        knowledge_content += f"URL: {source.get('url', 'N/A')}\n"
        knowledge_content += f"Dominio: {source.get('source_domain', 'N/A')}\n"
        # Mostrar scores multidimensionales si est√°n disponibles
        if all(
            key in source
            for key in [
                "authenticity_score",
                "reliability_score",
                "relevance_score",
                "currency_score",
            ]
        ):
            knowledge_content += f"Authenticity: {source.get('authenticity_score', 'N/A')}/10\n"
            knowledge_content += f"Reliability: {source.get('reliability_score', 'N/A')}/10\n"
            knowledge_content += f"Relevance: {source.get('relevance_score', 'N/A')}/10\n"
            knowledge_content += f"Currency: {source.get('currency_score', 'N/A')}/10\n"
            knowledge_content += (
                f"Total Score: {source.get('total_score', source.get('score', 'N/A'))}/10\n"
            )
            if source.get("is_clickbait") is not None:
                knowledge_content += f"Clickbait: {'S√≠' if source.get('is_clickbait') else 'No'}\n"
        else:
            knowledge_content += (
                f"Score de Calidad: {source.get('score', source.get('total_score', 'N/A'))}/10\n"
            )
        # Usar 'reasoning' si est√° disponible, sino 'reason' (compatibilidad)
        reasoning = source.get("reasoning", source.get("reason", "N/A"))
        knowledge_content += f"Raz√≥n de Aceptaci√≥n: {reasoning}\n"
        content = source.get('raw_content', source.get('snippet', 'N/A'))[:2000]
        knowledge_content += f"\nContenido:\n{content}\n"
        knowledge_content += "\n" + "-" * 80 + "\n\n"

    system_msg = """Eres un Analista de Investigaci√≥n Senior especializado en s√≠ntesis estrat√©gica.
Tu misi√≥n es analizar TODA la informaci√≥n recopilada y generar un informe estrat√©gico detallado y extenso.

INSTRUCCIONES CR√çTICAS:
1. Lee y analiza TODA la informaci√≥n proporcionada. No dejes nada fuera.
2. Identifica patrones, tendencias y conexiones entre las diferentes fuentes.
3. Usa referencias cruzadas entre fuentes para validar y enriquecer el an√°lisis.
4. Estructura el informe de manera profesional y estrat√©gica.
5. Incluye citas y referencias a las fuentes cuando sea relevante.
6. Proporciona insights accionables y conclusiones estrat√©gicas.
7. El informe debe ser extenso y detallado, aprovechando toda la informaci√≥n disponible.

FORMATO DEL INFORME:
- Resumen Ejecutivo
- An√°lisis Detallado (con subsecciones seg√∫n corresponda)
- Tendencias y Patrones Identificados
- Referencias Cruzadas entre Fuentes
- Conclusiones Estrat√©gicas
- Recomendaciones (si aplica)

Escribe en formato Markdown para facilitar la lectura."""

    user_msg = f"""{knowledge_content}

Genera un informe estrat√©gico completo y detallado que sintetice TODA esta informaci√≥n.
Aprovecha la capacidad de contexto para hacer referencias cruzadas y an√°lisis profundo."""

    try:
        # Seleccionar modelo: Gemini 2.5 Pro para s√≠ntesis final (reduce)
        # Esta es la s√≠ntesis final con evidencia ya filtrada - usar Gemini para m√°xima calidad
        llm = llm_analyst_fast if llm_analyst_fast is not None else llm_analyst
        print(f"      üìä Enviando {len(knowledge_content)} caracteres de contexto a Gemini 2.5 Pro (s√≠ntesis final)...")
        response = await llm.ainvoke(
            [{"role": "system", "content": system_msg}, {"role": "user", "content": user_msg}]
        )

        report = response.content if hasattr(response, "content") else str(response)

        # Agregar metadatos al inicio del reporte
        final_report = f"# Reporte de Investigaci√≥n: {topic}\n\n"
        final_report += f"**Fecha:** {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n"
        final_report += f"**Fuentes analizadas:** {len(knowledge_base)}\n\n"
        final_report += "---\n\n"
        final_report += report

        print(f"      ‚úÖ S√≠ntesis generada exitosamente ({len(final_report)} caracteres)")
        return final_report

    except Exception as e:
        print(f"      ‚ùå Error al generar s√≠ntesis con Gemini: {e}")
        # Fallback: usar el reporte Markdown simple
        print(f"      ‚ö†Ô∏è Usando reporte Markdown simple como fallback...")
        return await generate_markdown_report(
            topic=topic, all_sources=knowledge_base, report_type="General"
        )
